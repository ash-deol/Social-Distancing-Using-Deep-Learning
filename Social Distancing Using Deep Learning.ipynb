{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66277ff4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output layers ['yolo_82', 'yolo_94', 'yolo_106']\n"
     ]
    }
   ],
   "source": [
    "# Importing required libraries\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "# Derive the paths to the YOLO weights and model configuration\n",
    "# Load our YOLO object detector trained on COCO dataset\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "# Using GPU for the processing\n",
    "# set CUDA as the preferable backend and target\n",
    "net.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)\n",
    "net.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)\n",
    "\n",
    "# Set the distance threshold to 50 pixels.\n",
    "distance_thres = 50\n",
    "\n",
    "# Instantiate the VideoCapture object which will help us in reading frames from the video file.\n",
    "cap = cv2.VideoCapture('data/people1.webm')\n",
    "\n",
    "# A simple distance function which calculates the distance between two coordinates on the plane.\n",
    "def dist(x,y):\n",
    "    try:\n",
    "        return ((x[0]-y[0])**2 + (x[1]-y[1])**2)**0.5\n",
    "    except:\n",
    "        return\n",
    "\n",
    "# YOLOv3 has 3 output layers (82, 94 and 106) as the figure shows.\n",
    "# getLayerNames(): Get the name of all layers of the network.\n",
    "# getUnconnectedOutLayers(): Get the index of the output layers.\n",
    "# determine only the *output* layer names that we need from YOLO.\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "print('Output layers',output_layers)\n",
    "\n",
    "# Read a frame from the video just to get the height and width of it.\n",
    "_,frame = cap.read()\n",
    "\n",
    "# We will be saving our results in a video output also as shown below using VideoWriter.\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
    "writer = cv2.VideoWriter('output.avi', fourcc, 30,(frame.shape[1], frame.shape[0]), True)\n",
    "\n",
    "\n",
    "# Letâ€™s start the loop.\n",
    "ret = True\n",
    "while ret:\n",
    "\n",
    "    # Start reading from the input video.\n",
    "    ret,img = cap.read()\n",
    "    # If the cam object is returning something then the ret will be True.\n",
    "    if ret:\n",
    "        # Extract image height and width.\n",
    "        height, width = img.shape[:2]\n",
    "\n",
    "        # Create a blob of shape 416X416 from the image.\n",
    "        # Construct a blob from the input frame and then perform a forward\n",
    "        # pass of the YOLO object detector, giving us our bounding boxes\n",
    "        # and associated probabilities\n",
    "        blob = cv2.dnn.blobFromImage(img, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "\n",
    "        # Give this blob as input to the net using cv2.dnn.blobFromImage.\n",
    "        net.setInput(blob)\n",
    "        # Get output from output layers.\n",
    "        outs = net.forward(output_layers)\n",
    "\n",
    "        # Traverse in all the outputs from that frame.\n",
    "        # Now traverse in all the detections.\n",
    "        confidences = []\n",
    "        boxes = []\n",
    "        \n",
    "        for out in outs:\n",
    "            for detection in out:\n",
    "                # There are 85 points in the detection array. \n",
    "                # The first four indices are for coordinates of the box and indexes \n",
    "                # starting from 5 till 85 are for class confidences.\n",
    "                scores = detection[5:]\n",
    "                # Get the class id by getting the index of that element that has the highest score.\n",
    "                class_id = np.argmax(scores)\n",
    "                # If the class_id is not 0 (person), continue. \n",
    "                # Because our main purpose in this use case is just to detect persons.\n",
    "                if class_id!=0:\n",
    "                    continue\n",
    "                # Get the confidence score.\n",
    "                confidence = scores[class_id]\n",
    "                # If the confidence is greater than 30%, proceed further.\n",
    "                if confidence > 0.3:\n",
    "                    # Scale the bounding box coordinates back relative to the size of the image\n",
    "                    # YOLO actually returns the center (x, y)-coordinates of\n",
    "                    # the bounding box followed by the boxe's width and height\n",
    "                    # Calculate the center x and center y points.\n",
    "                    center_x = int(detection[0] * width)\n",
    "                    center_y = int(detection[1] * height)\n",
    "\n",
    "                    # Calculate the x,y,w,h of the bounding box.\n",
    "                    w = int(detection[2] * width)\n",
    "                    h = int(detection[3] * height)\n",
    "                    x = int(center_x - w / 2)\n",
    "                    y = int(center_y - h / 2)\n",
    "\n",
    "                    # Append this Bounding Box in our boxes list.\n",
    "                    boxes.append([x, y, w, h])\n",
    "                    # Append confidences in the confidences list.\n",
    "                    confidences.append(float(confidence))\n",
    "\n",
    "        # Apply non-maxima suppression to suppress weak, overlapping bounding boxes \n",
    "        # Here we are performing non-maximum suppression of bounding boxes using cv2.dnn.NMSBoxes. \n",
    "        # It will return a list of indexes containing a list of those indexes which we have to consider.\n",
    "        indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "        persons = []\n",
    "        person_centres = []\n",
    "        violate = set()\n",
    "\n",
    "        # Now traverse in all boxes and take only those boxes whose index is in the indexes list. \n",
    "        # And append-only these relevant boxes in the persons list. Also, append the box centers in the person_centres array.\n",
    "        for i in range(len(boxes)):\n",
    "            if i in indexes:\n",
    "                x,y,w,h = boxes[i]\n",
    "                persons.append(boxes[i])\n",
    "                person_centres.append([x+w//2,y+h//2])\n",
    "\n",
    "        # Now traverse in person_centres array and find all those person centers who are violating social distancing \n",
    "        # norms of 50 pixels. We will pass these person_centres points in the distance function and check the distances \n",
    "        # between all the persons. We will add these violating persons in the violate array.        \n",
    "        for i in range(len(persons)):\n",
    "            for j in range(i+1,len(persons)):\n",
    "                if dist(person_centres[i],person_centres[j]) <= distance_thres:\n",
    "                    violate.add(tuple(persons[i]))\n",
    "                    violate.add(tuple(persons[j]))\n",
    "        \n",
    "        # Simply draw a red box around the persons who are violating the social distancing norms and a green box \n",
    "        # around those who are not violating them.\n",
    "        v = 0\n",
    "        for (x,y,w,h) in persons:\n",
    "            if (x,y,w,h) in violate:\n",
    "                color = (0,0,255)\n",
    "                v+=1\n",
    "            else:\n",
    "                color = (0,255,0)\n",
    "            cv2.rectangle(img,(x,y),(x+w,y+h),color,2)\n",
    "            cv2.circle(img,(x+w//2,y+h//2),2,(0,0,255),2)\n",
    "\n",
    "        # Show no of violations on the screen.\n",
    "        cv2.putText(img,'No of Violations : '+str(v),(15,frame.shape[0]-10),cv2.FONT_HERSHEY_SIMPLEX,1,(0,126,255),2)\n",
    "        # Save the output in video form.\n",
    "        writer.write(img)\n",
    "        # Showing the output.\n",
    "        cv2.imshow(\"Image\", img)\n",
    "    # If anyone hits the ESC key, break the code.\n",
    "    if cv2.waitKey(1) == 27:\n",
    "        break\n",
    "\n",
    "# Release the VideoCapture object and destroy all open windows.\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
